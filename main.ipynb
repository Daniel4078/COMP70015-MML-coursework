{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first write down the models that we are going to try out here.\n",
    "Done:\n",
    "We currently have K-NN with feature pruning that remove ones with high variance, probabily due to input are bipolar and some of them are providing redundant info causing distance difference between points in higher dimension become smaller and so KNN performs not well with those extra feature\n",
    "Also show that random forest/decision tree is not suitable for this task as we found the features do not provide a binary separation for their labels as we tried to convert all input into binary(true if it is bigger than the mean of the features) as we found all feature's values form normal distribution with two peaks.\n",
    "TODO:\n",
    "Going to try engineer a different prompt for GPT2 to counter-act its large language model behavior to reduce hallucination. try to reduce size of test batch to deal with language model's limit memory on context?\n",
    "try basis expansion with other basis than polynomial (like logrismic and sin/cosin using scikit-learn's kernel_pca) and explain why polynomial expansion is not suited.\n",
    "try implement a simple neural network using DEHB for auto hyperparameter tuning. https://github.com/automl/DEHB?tab=readme-ov-file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
